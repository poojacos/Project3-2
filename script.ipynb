{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example:\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n",
    "    \"\"\"\n",
    "    See: https://stackoverflow.com/a/26980472\n",
    "\n",
    "    Identify most important features if given a vectorizer and binary classifier. Set n to the number\n",
    "    of weighted features you would like to show. (Note: current implementation merely prints and does not\n",
    "    return top classes.)\n",
    "    \"\"\"\n",
    "\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "\n",
    "    print()\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "\n",
    "\n",
    "def plot_history_2win(history):\n",
    "    plt.subplot(211)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['acc'], color='g', label='Train')\n",
    "    plt.plot(history.history['val_acc'], color='b', label='Validation')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], color='g', label='Train')\n",
    "    plt.plot(history.history['val_loss'], color='b', label='Validation')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_history_plot(history, model_name, metrics=None):\n",
    "    plt.title('Accuracy and Loss (' + model_name + ')')\n",
    "    if metrics is None:\n",
    "        metrics = {'acc', 'loss'}\n",
    "    if 'acc' in metrics:\n",
    "        plt.plot(history.history['acc'], color='g', label='Train Accuracy')\n",
    "        plt.plot(history.history['val_acc'], color='b', label='Validation Accuracy')\n",
    "    if 'loss' in metrics:\n",
    "        plt.plot(history.history['loss'], color='r', label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], color='m', label='Validation Loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_history(history, model_name):\n",
    "    create_history_plot(history, model_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_and_save_history(history, model_name, file_path, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = {'acc', 'loss'}\n",
    "    create_history_plot(history, model_name, metrics)\n",
    "    plt.savefig(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 5000\n",
    "MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras_text_summarization.library.utility.glove_loader import load_glove, GLOVE_EMBEDDING_SIZE\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "DEFAULT_BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "DEFAULT_EPOCHS = 10\n",
    "\n",
    "\n",
    "class Seq2SeqSummarizer(object):\n",
    "\n",
    "    model_name = 'seq2seq'\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.num_input_tokens = config['num_input_tokens']\n",
    "        self.max_input_seq_length = config['max_input_seq_length']\n",
    "        self.num_target_tokens = config['num_target_tokens']\n",
    "        self.max_target_seq_length = config['max_target_seq_length']\n",
    "        self.input_word2idx = config['input_word2idx']\n",
    "        self.input_idx2word = config['input_idx2word']\n",
    "        self.target_word2idx = config['target_word2idx']\n",
    "        self.target_idx2word = config['target_idx2word']\n",
    "        self.config = config\n",
    "\n",
    "        self.version = 0\n",
    "        if 'version' in config:\n",
    "            self.version = config['version']\n",
    "\n",
    "        encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding = Embedding(input_dim=self.num_input_tokens, output_dim=HIDDEN_UNITS,\n",
    "                                      input_length=self.max_input_seq_length, name='encoder_embedding')\n",
    "        encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n",
    "        decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                         initial_state=encoder_states)\n",
    "        decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "        decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "    def load_weights(self, weight_file_path):\n",
    "        if os.path.exists(weight_file_path):\n",
    "            self.model.load_weights(weight_file_path)\n",
    "\n",
    "    def transform_input_text(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            for word in line.lower().split(' '):\n",
    "                wid = 1\n",
    "                if word in self.input_word2idx:\n",
    "                    wid = self.input_word2idx[word]\n",
    "                x.append(wid)\n",
    "                if len(x) >= self.max_input_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "        temp = pad_sequences(temp, maxlen=self.max_input_seq_length)\n",
    "\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def transform_target_encoding(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            line2 = 'START ' + line.lower() + ' END'\n",
    "            for word in line2.split(' '):\n",
    "                x.append(word)\n",
    "                if len(x) >= self.max_target_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "\n",
    "        temp = np.array(temp)\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "        num_batches = len(x_samples) // batch_size\n",
    "        while True:\n",
    "            for batchIdx in range(0, num_batches):\n",
    "                start = batchIdx * batch_size\n",
    "                end = (batchIdx + 1) * batch_size\n",
    "                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n",
    "                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                    for idx, w in enumerate(target_words):\n",
    "                        w2idx = 0  # default [UNK]\n",
    "                        if w in self.target_word2idx:\n",
    "                            w2idx = self.target_word2idx[w]\n",
    "                        if w2idx != 0:\n",
    "                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                            if idx > 0:\n",
    "                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_weight_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-weights.h5'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-config.npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_architecture_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-architecture.json'\n",
    "\n",
    "    def fit(self, Xtrain, Ytrain, Xtest, Ytest, epochs=None, batch_size=None, model_dir_path=None):\n",
    "        if epochs is None:\n",
    "            epochs = DEFAULT_EPOCHS\n",
    "        if model_dir_path is None:\n",
    "            model_dir_path = './'\n",
    "        if batch_size is None:\n",
    "            batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "        self.version += 1\n",
    "        self.config['version'] = self.version\n",
    "        config_file_path = Seq2SeqSummarizer.get_config_file_path(model_dir_path)\n",
    "        weight_file_path = Seq2SeqSummarizer.get_weight_file_path(model_dir_path)\n",
    "        checkpoint = ModelCheckpoint(weight_file_path)\n",
    "        np.save(config_file_path, self.config)\n",
    "        architecture_file_path = Seq2SeqSummarizer.get_architecture_file_path(model_dir_path)\n",
    "        open(architecture_file_path, 'w').write(self.model.to_json())\n",
    "\n",
    "        Ytrain = self.transform_target_encoding(Ytrain)\n",
    "        Ytest = self.transform_target_encoding(Ytest)\n",
    "\n",
    "        Xtrain = self.transform_input_text(Xtrain)\n",
    "        Xtest = self.transform_input_text(Xtest)\n",
    "\n",
    "        train_gen = self.generate_batch(Xtrain, Ytrain, batch_size)\n",
    "        test_gen = self.generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "        train_num_batches = len(Xtrain) // batch_size\n",
    "        test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs=epochs,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])\n",
    "        self.model.save_weights(weight_file_path)\n",
    "        return history\n",
    "\n",
    "    def summarize(self, input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in input_text.lower().split(' '):\n",
    "            idx = 1  # default [UNK]\n",
    "            if word in self.input_word2idx:\n",
    "                idx = self.input_word2idx[word]\n",
    "            input_wids.append(idx)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv file ...\n",
      "extract configuration from input texts ...\n",
      "(5068,)\n",
      "(1267,)\n",
      "(5068, 500)\n",
      "(1267, 500)\n",
      "Epoch 1/80\n",
      "79/79 [==============================] - 69s - loss: 1.0834 - acc: 0.0215 - val_loss: 1.0295 - val_acc: 0.0223\n",
      "Epoch 2/80\n",
      "79/79 [==============================] - 68s - loss: 1.0389 - acc: 0.0226 - val_loss: 1.0245 - val_acc: 0.0223\n",
      "Epoch 3/80\n",
      "79/79 [==============================] - 68s - loss: 1.0320 - acc: 0.0228 - val_loss: 1.0213 - val_acc: 0.0221\n",
      "Epoch 4/80\n",
      "79/79 [==============================] - 68s - loss: 1.0269 - acc: 0.0228 - val_loss: 1.0187 - val_acc: 0.0222\n",
      "Epoch 5/80\n",
      "79/79 [==============================] - 68s - loss: 1.0217 - acc: 0.0230 - val_loss: 1.0167 - val_acc: 0.0222\n",
      "Epoch 6/80\n",
      "79/79 [==============================] - 67s - loss: 1.0165 - acc: 0.0232 - val_loss: 1.0141 - val_acc: 0.0224\n",
      "Epoch 7/80\n",
      "79/79 [==============================] - 68s - loss: 1.0107 - acc: 0.0235 - val_loss: 1.0118 - val_acc: 0.0227\n",
      "Epoch 8/80\n",
      "79/79 [==============================] - 67s - loss: 1.0046 - acc: 0.0240 - val_loss: 1.0097 - val_acc: 0.0231\n",
      "Epoch 9/80\n",
      "79/79 [==============================] - 67s - loss: 0.9969 - acc: 0.0247 - val_loss: 1.0087 - val_acc: 0.0231\n",
      "Epoch 10/80\n",
      "79/79 [==============================] - 67s - loss: 0.9886 - acc: 0.0256 - val_loss: 1.0048 - val_acc: 0.0237\n",
      "Epoch 11/80\n",
      "79/79 [==============================] - 67s - loss: 0.9791 - acc: 0.0263 - val_loss: 0.9978 - val_acc: 0.0245\n",
      "Epoch 12/80\n",
      "79/79 [==============================] - 67s - loss: 0.9690 - acc: 0.0271 - val_loss: 0.9927 - val_acc: 0.0247\n",
      "Epoch 13/80\n",
      "79/79 [==============================] - 67s - loss: 0.9576 - acc: 0.0278 - val_loss: 0.9895 - val_acc: 0.0251\n",
      "Epoch 14/80\n",
      "79/79 [==============================] - 67s - loss: 0.9470 - acc: 0.0286 - val_loss: 0.9838 - val_acc: 0.0255\n",
      "Epoch 15/80\n",
      "79/79 [==============================] - 67s - loss: 0.9353 - acc: 0.0294 - val_loss: 0.9812 - val_acc: 0.0256\n",
      "Epoch 16/80\n",
      "79/79 [==============================] - 67s - loss: 0.9247 - acc: 0.0299 - val_loss: 0.9807 - val_acc: 0.0265\n",
      "Epoch 17/80\n",
      "79/79 [==============================] - 67s - loss: 0.9035 - acc: 0.0314 - val_loss: 0.9755 - val_acc: 0.0267\n",
      "Epoch 19/80\n",
      "79/79 [==============================] - 67s - loss: 0.8937 - acc: 0.0323 - val_loss: 0.9688 - val_acc: 0.0270\n",
      "Epoch 20/80\n",
      "79/79 [==============================] - 67s - loss: 0.8831 - acc: 0.0330 - val_loss: 0.9675 - val_acc: 0.0263\n",
      "Epoch 21/80\n",
      "79/79 [==============================] - 67s - loss: 0.8733 - acc: 0.0339 - val_loss: 0.9687 - val_acc: 0.0253\n",
      "Epoch 22/80\n",
      "79/79 [==============================] - 67s - loss: 0.8636 - acc: 0.0345 - val_loss: 0.9658 - val_acc: 0.0254\n",
      "Epoch 23/80\n",
      "79/79 [==============================] - 67s - loss: 0.8532 - acc: 0.0354 - val_loss: 0.9677 - val_acc: 0.0270\n",
      "Epoch 24/80\n",
      "79/79 [==============================] - 67s - loss: 0.8439 - acc: 0.0359 - val_loss: 0.9716 - val_acc: 0.0253\n",
      "Epoch 25/80\n",
      "79/79 [==============================] - 67s - loss: 0.8344 - acc: 0.0369 - val_loss: 0.9665 - val_acc: 0.0268\n",
      "Epoch 26/80\n",
      "79/79 [==============================] - 67s - loss: 0.8242 - acc: 0.0376 - val_loss: 0.9677 - val_acc: 0.0261\n",
      "Epoch 27/80\n",
      "79/79 [==============================] - 67s - loss: 0.8141 - acc: 0.0385 - val_loss: 0.9630 - val_acc: 0.0285\n",
      "Epoch 28/80\n",
      "79/79 [==============================] - 67s - loss: 0.8047 - acc: 0.0393 - val_loss: 0.9668 - val_acc: 0.0269\n",
      "Epoch 29/80\n",
      "79/79 [==============================] - 67s - loss: 0.7955 - acc: 0.0401 - val_loss: 0.9700 - val_acc: 0.0266\n",
      "Epoch 30/80\n",
      "79/79 [==============================] - 67s - loss: 0.7852 - acc: 0.0408 - val_loss: 0.9720 - val_acc: 0.0278\n",
      "Epoch 31/80\n",
      "79/79 [==============================] - 67s - loss: 0.7765 - acc: 0.0418 - val_loss: 0.9781 - val_acc: 0.0263\n",
      "Epoch 32/80\n",
      "79/79 [==============================] - 67s - loss: 0.7667 - acc: 0.0427 - val_loss: 0.9730 - val_acc: 0.0273\n",
      "Epoch 33/80\n",
      "79/79 [==============================] - 67s - loss: 0.7568 - acc: 0.0435 - val_loss: 0.9751 - val_acc: 0.0257\n",
      "Epoch 34/80\n",
      "79/79 [==============================] - 67s - loss: 0.7476 - acc: 0.0444 - val_loss: 0.9808 - val_acc: 0.0252\n",
      "Epoch 35/80\n",
      "79/79 [==============================] - 67s - loss: 0.7383 - acc: 0.0455 - val_loss: 0.9817 - val_acc: 0.0278\n",
      "Epoch 36/80\n",
      "79/79 [==============================] - 67s - loss: 0.7281 - acc: 0.0466 - val_loss: 0.9869 - val_acc: 0.0253\n",
      "Epoch 37/80\n",
      "79/79 [==============================] - 67s - loss: 0.7184 - acc: 0.0475 - val_loss: 0.9906 - val_acc: 0.0255\n",
      "Epoch 38/80\n",
      "79/79 [==============================] - 67s - loss: 0.7101 - acc: 0.0484 - val_loss: 0.9909 - val_acc: 0.0256\n",
      "Epoch 39/80\n",
      "79/79 [==============================] - 67s - loss: 0.7002 - acc: 0.0493 - val_loss: 1.0008 - val_acc: 0.0250\n",
      "Epoch 40/80\n",
      "79/79 [==============================] - 67s - loss: 0.6908 - acc: 0.0507 - val_loss: 0.9992 - val_acc: 0.0269\n",
      "Epoch 41/80\n",
      "79/79 [==============================] - 67s - loss: 0.6805 - acc: 0.0517 - val_loss: 1.0064 - val_acc: 0.0242\n",
      "Epoch 42/80\n",
      "79/79 [==============================] - 66s - loss: 0.6720 - acc: 0.0524 - val_loss: 1.0123 - val_acc: 0.0247\n",
      "Epoch 43/80\n",
      "79/79 [==============================] - 66s - loss: 0.6624 - acc: 0.0537 - val_loss: 1.0216 - val_acc: 0.0256\n",
      "Epoch 44/80\n",
      "79/79 [==============================] - 66s - loss: 0.6533 - acc: 0.0547 - val_loss: 1.0232 - val_acc: 0.0248\n",
      "Epoch 45/80\n",
      "79/79 [==============================] - 67s - loss: 0.6444 - acc: 0.0558 - val_loss: 1.0279 - val_acc: 0.0250\n",
      "Epoch 46/80\n",
      "79/79 [==============================] - 67s - loss: 0.6351 - acc: 0.0569 - val_loss: 1.0363 - val_acc: 0.0254\n",
      "Epoch 47/80\n",
      "79/79 [==============================] - 66s - loss: 0.6261 - acc: 0.0581 - val_loss: 1.0398 - val_acc: 0.0237\n",
      "Epoch 48/80\n",
      "79/79 [==============================] - 67s - loss: 0.6182 - acc: 0.0589 - val_loss: 1.0450 - val_acc: 0.0245\n",
      "Epoch 49/80\n",
      "79/79 [==============================] - 67s - loss: 0.6083 - acc: 0.0604 - val_loss: 1.0488 - val_acc: 0.0245\n",
      "Epoch 50/80\n",
      "79/79 [==============================] - 67s - loss: 0.5998 - acc: 0.0614 - val_loss: 1.0506 - val_acc: 0.0245\n",
      "Epoch 51/80\n",
      "79/79 [==============================] - 66s - loss: 0.5910 - acc: 0.0625 - val_loss: 1.0610 - val_acc: 0.0228\n",
      "Epoch 52/80\n",
      "79/79 [==============================] - 67s - loss: 0.5827 - acc: 0.0636 - val_loss: 1.0681 - val_acc: 0.0244\n",
      "Epoch 53/80\n",
      "79/79 [==============================] - 67s - loss: 0.5729 - acc: 0.0651 - val_loss: 1.0718 - val_acc: 0.0234\n",
      "Epoch 54/80\n",
      "79/79 [==============================] - 67s - loss: 0.5651 - acc: 0.0660 - val_loss: 1.0800 - val_acc: 0.0241\n",
      "Epoch 55/80\n",
      "79/79 [==============================] - 66s - loss: 0.5564 - acc: 0.0672 - val_loss: 1.0831 - val_acc: 0.0225\n",
      "Epoch 56/80\n",
      "79/79 [==============================] - 67s - loss: 0.5478 - acc: 0.0691 - val_loss: 1.0901 - val_acc: 0.0233\n",
      "Epoch 57/80\n",
      "79/79 [==============================] - 66s - loss: 0.5399 - acc: 0.0699 - val_loss: 1.1022 - val_acc: 0.0244\n",
      "Epoch 58/80\n",
      "79/79 [==============================] - 67s - loss: 0.5311 - acc: 0.0709 - val_loss: 1.0998 - val_acc: 0.0221\n",
      "Epoch 59/80\n",
      "79/79 [==============================] - 67s - loss: 0.5235 - acc: 0.0726 - val_loss: 1.1040 - val_acc: 0.0225\n",
      "Epoch 60/80\n",
      "79/79 [==============================] - 66s - loss: 0.5158 - acc: 0.0734 - val_loss: 1.1151 - val_acc: 0.0226\n",
      "Epoch 61/80\n",
      "79/79 [==============================] - 67s - loss: 0.5068 - acc: 0.0749 - val_loss: 1.1203 - val_acc: 0.0233\n",
      "Epoch 62/80\n",
      "79/79 [==============================] - 66s - loss: 0.4998 - acc: 0.0758 - val_loss: 1.1258 - val_acc: 0.0219\n",
      "Epoch 63/80\n",
      "79/79 [==============================] - 66s - loss: 0.4909 - acc: 0.0775 - val_loss: 1.1368 - val_acc: 0.0230\n",
      "Epoch 64/80\n",
      "79/79 [==============================] - 67s - loss: 0.4843 - acc: 0.0784 - val_loss: 1.1375 - val_acc: 0.0233\n",
      "Epoch 65/80\n",
      "79/79 [==============================] - 67s - loss: 0.4755 - acc: 0.0798 - val_loss: 1.1407 - val_acc: 0.0225\n",
      "Epoch 66/80\n",
      "79/79 [==============================] - 66s - loss: 0.4682 - acc: 0.0815 - val_loss: 1.1519 - val_acc: 0.0216\n",
      "Epoch 67/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 67s - loss: 0.4606 - acc: 0.0821 - val_loss: 1.1547 - val_acc: 0.0226\n",
      "Epoch 68/80\n",
      "79/79 [==============================] - 66s - loss: 0.4537 - acc: 0.0836 - val_loss: 1.1647 - val_acc: 0.0219\n",
      "Epoch 69/80\n",
      "79/79 [==============================] - 66s - loss: 0.4456 - acc: 0.0850 - val_loss: 1.1721 - val_acc: 0.0220\n",
      "Epoch 70/80\n",
      "79/79 [==============================] - 66s - loss: 0.4384 - acc: 0.0865 - val_loss: 1.1792 - val_acc: 0.0213\n",
      "Epoch 71/80\n",
      "79/79 [==============================] - 67s - loss: 0.4314 - acc: 0.0877 - val_loss: 1.1770 - val_acc: 0.0221\n",
      "Epoch 72/80\n",
      "79/79 [==============================] - 66s - loss: 0.4233 - acc: 0.0892 - val_loss: 1.1874 - val_acc: 0.0217\n",
      "Epoch 73/80\n",
      "79/79 [==============================] - 66s - loss: 0.4171 - acc: 0.0907 - val_loss: 1.1966 - val_acc: 0.0209\n",
      "Epoch 74/80\n",
      "79/79 [==============================] - 66s - loss: 0.4098 - acc: 0.0917 - val_loss: 1.1957 - val_acc: 0.0212\n",
      "Epoch 75/80\n",
      "79/79 [==============================] - 66s - loss: 0.4030 - acc: 0.0930 - val_loss: 1.2054 - val_acc: 0.0223\n",
      "Epoch 76/80\n",
      "79/79 [==============================] - 66s - loss: 0.3962 - acc: 0.0944 - val_loss: 1.2157 - val_acc: 0.0215\n",
      "Epoch 77/80\n",
      "79/79 [==============================] - 67s - loss: 0.3888 - acc: 0.0956 - val_loss: 1.2176 - val_acc: 0.0206\n",
      "Epoch 78/80\n",
      "79/79 [==============================] - 66s - loss: 0.3823 - acc: 0.0969 - val_loss: 1.2270 - val_acc: 0.0216\n",
      "Epoch 79/80\n",
      "79/79 [==============================] - 67s - loss: 0.3757 - acc: 0.0985 - val_loss: 1.2279 - val_acc: 0.0211\n",
      "Epoch 80/80\n",
      "79/79 [==============================] - 66s - loss: 0.3692 - acc: 0.0995 - val_loss: 1.2327 - val_acc: 0.0210\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'report_dir_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d7f09cbe6adf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mhistory_plot_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_dir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSeq2SeqSummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-history.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLOAD_EXISTING_WEIGHTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mhistory_plot_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_dir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSeq2SeqSummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-history-v'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'report_dir_path' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "LOAD_EXISTING_WEIGHTS = False\n",
    "\n",
    "np.random.seed(42)\n",
    "data_dir_path = './'\n",
    "model_dir_path = './'\n",
    "\n",
    "print('loading csv file ...')\n",
    "df = pd.read_csv(data_dir_path + \"/fake_or_real_news.csv\")\n",
    "\n",
    "print('extract configuration from input texts ...')\n",
    "Y = df.title\n",
    "X = df['text']\n",
    "\n",
    "config = fit_text(X, Y)\n",
    "\n",
    "summarizer = Seq2SeqSummarizer(config)\n",
    "\n",
    "if LOAD_EXISTING_WEIGHTS:\n",
    "    summarizer.load_weights(weight_file_path=Seq2SeqSummarizer.get_weight_file_path(model_dir_path=model_dir_path))\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "history = summarizer.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report_dir_path = './'\n",
    "LOAD_EXISTING_WEIGHTS = True\n",
    "history_plot_file_path = report_dir_path + '/' + Seq2SeqSummarizer.model_name + '-history.png'\n",
    "if LOAD_EXISTING_WEIGHTS:\n",
    "    history_plot_file_path = report_dir_path + '/' + Seq2SeqSummarizer.model_name + '-history-v' + str(summarizer.version) + '.png'\n",
    "plot_and_save_history(history, summarizer.model_name, history_plot_file_path, metrics={'loss', 'acc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv file ...\n",
      "start predicting ...\n",
      "Article:  Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \n",
      "In the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \n",
      "The word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \n",
      "But that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \n",
      "The FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \n",
      "The FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \n",
      "The covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \n",
      "The New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \n",
      "James Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \n",
      "Countless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \n",
      "Senator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \n",
      "If James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \n",
      "Not too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \n",
      "Either Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \n",
      "The only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \n",
      "And it’s an interesting question. \n",
      "Hillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \n",
      "Pretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \n",
      "There are two possible explanations. \n",
      "Hillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \n",
      "But the other explanation is that her people panicked. \n",
      "Going to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \n",
      "During the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \n",
      "There’s only one reason for such bizarre behavior. \n",
      "The Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \n",
      "Clinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \n",
      "You can smell the fear. \n",
      "The FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \n",
      "The misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \n",
      "The Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \n",
      "The campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \n",
      "Hillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.  \n",
      "Generated :  re: wikileaks emails exposed by fbi director information will not a russian – truthfeed\n",
      "Original Summary:  You Can Smell Hillary’s Fear\n",
      "Article:  Google Pinterest Digg Linkedin Reddit Stumbleupon Print Delicious Pocket Tumblr \n",
      "There are two fundamental truths in this world: Paul Ryan desperately wants to be president. And Paul Ryan will never be president. Today proved it. \n",
      "In a particularly staggering example of political cowardice, Paul Ryan re-re-re-reversed course and announced that he was back on the Trump Train after all. This was an aboutface from where he was a few weeks ago. He had previously declared he would not be supporting or defending Trump after a tape was made public in which Trump bragged about assaulting women. Suddenly, Ryan was appearing at a pro-Trump rally and boldly declaring that he already sent in his vote to make him President of the United States. It was a surreal moment. The figurehead of the Republican Party dosed himself in gasoline, got up on a stage on a chilly afternoon in Wisconsin, and lit a match. . @SpeakerRyan says he voted for @realDonaldTrump : “Republicans, it is time to come home” https://t.co/VyTT49YvoE pic.twitter.com/wCvSCg4a5I \n",
      "— ABC News Politics (@ABCPolitics) November 5, 2016 \n",
      "The Democratic Party couldn’t have asked for a better moment of film. Ryan’s chances of ever becoming president went down to zero in an instant. In the wreckage Trump is to leave behind in his wake, those who cravenly backed his campaign will not recover. If Ryan’s career manages to limp all the way to 2020, then the DNC will have this tape locked and loaded to be used in every ad until Election Day. \n",
      "The ringing endorsement of the man he clearly hates on a personal level speaks volumes about his own spinelessness. Ryan has postured himself as a “principled” conservative, and one uncomfortable with Trump’s unapologetic bigotry and sexism. However, when push came to shove, Paul Ryan – like many of his colleagues – turned into a sniveling appeaser. After all his lofty tak about conviction, his principles were a house of cards and collapsed with the slightest breeze. \n",
      "What’s especially bizarre is how close Ryan came to making it through unscathed. For months the Speaker of the House refused to comment on Trump at all. His strategy seemed to be to keep his head down, pretend Trump didn’t exist, and hope that nobody remembered what happened in 2016. Now, just days away from the election, he screwed it all up. \n",
      "If 2016’s very ugly election has done any good it’s by exposing the utter cowardice of the Republicans who once feigned moral courage. A reality television star spit on them, hijacked their party, insulted their wives, and got every last one of them to kneel before him. What a turn of events. \n",
      "Featured image via Twitter\n",
      "Generated :  hillary clinton campaign her her own she’s for the fbi will be a announcement at a home of this\n",
      "Original Summary:  Watch The Exact Moment Paul Ryan Committed Political Suicide At A Trump Rally (VIDEO)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:  U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday’s unity march against terrorism.\n",
      "\n",
      "Kerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\n",
      "\n",
      "The visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\n",
      "\n",
      "The French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday’s march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\n",
      "\n",
      "Among roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\n",
      "\n",
      "Kerry spent Sunday at a business summit hosted by India’s prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi’s government will act to open the huge Indian market for more American businesses.\n",
      "\n",
      "In a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as “quibbling a little bit.” He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\n",
      "\n",
      "“But that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,” he said.\n",
      "\n",
      "“And I don’t think the people of France have any doubts about America’s understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.”\n",
      "Generated :  how to go to trump’s with with trump\n",
      "Original Summary:  Kerry to go to Paris in gesture of sympathy\n",
      "Article:  — Kaydee King (@KaydeeKing) November 9, 2016 The lesson from tonight's Dem losses: Time for Democrats to start listening to the voters. Stop running the same establishment candidates. \n",
      "— People For Bernie (@People4Bernie) November 9, 2016 If Dems didn't want a tight race they shouldn't have worked against Bernie. \n",
      "— Walker Bragman (@WalkerBragman) November 9, 2016 \n",
      "New York Times columnist Paul Krugman, who was one of Hillary Clinton’s most outspoken surrogates during the contentious Democratic primary, blamed Clinton’s poor performance on Green Party candidate Jill Stein, who has so far received a negligible number of votes nationally, saying Stein was the Ralph Nader of 2016 in preventing a Clinton victory. The account @BerniesTeachers threw Krugman’s analysis back in his face. Your candidate was the issue. Take responsibility. https://t.co/KHyOuUSrFS \n",
      "— Teachers for Bernie (@BerniesTeachers) November 9, 2016 \n",
      "Ana Navarro, a Republican who recently endorsed Hillary Clinton, summed up the preposterous nature of the 2016 presidential election in this tweet: GOP nominated the only damn candidate who could lose to Hillary Clinton. Democrats nominated the only damn candidate who could lose to Trump \n",
      "— Ana Navarro (@ananavarro) November 9, 2016 \n",
      "Popular left-wing Facebook page The Other 98%, which was pro-Sanders during the primary, responded to Trump’s surge by simply posting a meme of Sanders’ face with the text “All this could’ve been avoided. Thanks for nothing, DNC!” The meme has been shared almost 15,000 times in less than an hour: \n",
      "Posted by The Other 98% on Tuesday, November 8, 2016 \n",
      "While Bernie Sanders endorsed Hillary Clinton just before the Democratic National Convention in July, many of his supporters remained adamant in their refusal to support the DNC-anointed candidate, pointing to WikiLeaks’ revelations that top officials at the DNC had been working behind the scenes to tip the scales in Clinton’s favor by coordinating with media figures to circulate anti-Sanders narratives. \n",
      "Rather than attribute a potential Trump presidency to the GOP nominee’s perceived popularity among voters, the closeness of this election could be credited to Hillary Clinton’s unfavorable ratings. According to RealClearPolitics, anywhere between 51 and 57 percent of voters had a negative opinion of the Democratic nominee. \n",
      "As of 11 PM Eastern, Florida, Michigan, Pennsylvania, and Wisconsin remain too close to call. Clinton has 197 electoral votes to Trump’s 187. \n",
      "\n",
      "Zach Cartwright is an activist and author from Richmond, Virginia. He enjoys writing about politics, government, and the media. Send him an email at [email protected]\n",
      "Generated :  poll: clinton debate performance in iowa, to keep the new hampshire\n",
      "Original Summary:  Bernie supporters on Twitter erupt in anger against the DNC: 'We tried to warn you!'\n",
      "Article:  It's primary day in New York and front-runners Hillary Clinton and Donald Trump are leading in the polls.\n",
      "\n",
      "Trump is now vowing to win enough delegates to clinch the Republican nomination and prevent a contested convention. But Sens.Ted Cruz, R-Texas, Bernie Sanders, D-Vt., and Ohio Gov. John Kasich and aren't giving up just yet.\n",
      "\n",
      "A big win in New York could tip the scales for both the Republican and Democratic front-runners in this year's race for the White House. Clinton and Trump have each suffered losses in recent contests, shifting the momentum to their rivals.\n",
      "\n",
      "\"We have won eight out of the last nine caucuses and primaries! Cheer!\" Sanders recently told supporters.\n",
      "\n",
      "While wins in New York for Trump and Clinton are expected, the margins of those victories are also important.\n",
      "\n",
      "Trump needs to capture more than 50 percent of the vote statewide if he wants to be positioned to win all of the state's 95 GOP delegates. That would put him one step closer to avoiding a contested convention.\n",
      "\n",
      "\"We've got to vote and you know Cruz is way, way down in the polls,\" Trump urged supporters.\n",
      "\n",
      "Meanwhile, Sanders is hoping for a close race in the Empire State. A loss by 10 points means he'll need to win 80 percent of the remaining delegates to clinch the nomination.\n",
      "\n",
      "Despite a predicted loss in New York, Cruz hasn't lost momentum. He's hoping to sweep up more delegates this weekend while he's talking about how he can win in November.\n",
      "\n",
      "\"Because if I'm the nominee, we win the General Election,\" Cruz promised his supporters. \"We're beating Hillary in the key swing states, we're beating Hillary with Independents, we're beating Hillary with young people.\"\n",
      "\n",
      "For now, Cruz, Kasich, and Sanders have all moved on from New York to other states. Trump and Clinton are the only two staying in their home state to watch the results come in.\n",
      "Generated :  the daily 202: why paul is ryan\n",
      "Original Summary:  The Battle of New York: Why This Primary Matters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "data_dir_path = './' # refers to the demo/data folder\n",
    "model_dir_path = './' # refers to the demo/models folder\n",
    "\n",
    "print('loading csv file ...')\n",
    "df = pd.read_csv(\"./fake_or_real_news.csv\")\n",
    "X = df['text']\n",
    "Y = df.title\n",
    "\n",
    "config = np.load(Seq2SeqSummarizer.get_config_file_path(model_dir_path=model_dir_path)).item()\n",
    "\n",
    "summarizer = Seq2SeqSummarizer(config)\n",
    "summarizer.load_weights(weight_file_path=Seq2SeqSummarizer.get_weight_file_path(model_dir_path=model_dir_path))\n",
    "\n",
    "print('start predicting ...')\n",
    "for i in range(5):\n",
    "    x = X[i]\n",
    "    actual_headline = Y[i]\n",
    "    headline = summarizer.summarize(x)\n",
    "    print('Article: ', x)\n",
    "    print('Generated : ', headline)\n",
    "    print('Original Summary: ', actual_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
